{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b26f1be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/siddiquia0/anaconda3/envs/proj/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.layers import DropPath\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import _load_weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from timm.models.layers import DropPath\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout, out_dim=None):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        if out_dim is None:\n",
    "            out_dim = dim\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        head_dim = dim // heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.attn = None\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, N, C = x.shape\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B, N, 3, self.heads, C // self.heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q, k, v = (\n",
    "            qkv[0],\n",
    "            qkv[1],\n",
    "            qkv[2],\n",
    "        )\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_dim, dropout, drop_path):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, heads, dropout)\n",
    "        self.mlp = FeedForward(dim, mlp_dim, dropout)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x), mask)\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        trunc_normal_(m.weight, std=0.02)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "\n",
    "def resize_pos_embed(posemb, grid_old_shape, grid_new_shape, num_extra_tokens):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    posemb_tok, posemb_grid = (\n",
    "        posemb[:, :num_extra_tokens],\n",
    "        posemb[0, num_extra_tokens:],\n",
    "    )\n",
    "    if grid_old_shape is None:\n",
    "        gs_old_h = int(math.sqrt(len(posemb_grid)))\n",
    "        gs_old_w = gs_old_h\n",
    "    else:\n",
    "        gs_old_h, gs_old_w = grid_old_shape\n",
    "\n",
    "    gs_h, gs_w = grid_new_shape\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old_h, gs_old_w, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=(gs_h, gs_w), mode=\"bilinear\")\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_h * gs_w, -1)\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, embed_dim, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        if image_size[0] % patch_size != 0 or image_size[1] % patch_size != 0:\n",
    "            raise ValueError(\"image dimensions must be divisible by the patch size\")\n",
    "        self.grid_size = image_size[0] // patch_size, image_size[1] // patch_size\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.patch_size = patch_size\n",
    "        self.proj = nn.Conv2d(\n",
    "            channels, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, im):\n",
    "        B, C, H, W = im.shape\n",
    "        x = self.proj(im)\n",
    "        x= x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "def unpadding(y, target_size):\n",
    "    H, W = target_size\n",
    "    H_pad, W_pad = y.size(2), y.size(3)\n",
    "    # crop predictions on extra pixels coming from padding\n",
    "    extra_h = H_pad - H\n",
    "    extra_w = W_pad - W\n",
    "    if extra_h > 0:\n",
    "        y = y[:, :, :-extra_h]\n",
    "    if extra_w > 0:\n",
    "        y = y[:, :, :, :-extra_w]\n",
    "    return y\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        n_layers,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        n_heads,\n",
    "        n_cls,\n",
    "        dropout=0.1,\n",
    "        drop_path_rate=0.0,\n",
    "        distilled=False,\n",
    "        channels=3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            image_size,\n",
    "            patch_size,\n",
    "            d_model,\n",
    "            channels,\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.n_cls = n_cls\n",
    "\n",
    "        # cls and pos tokens\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.distilled = distilled\n",
    "        if self.distilled:\n",
    "            self.dist_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.randn(1, self.patch_embed.num_patches + 2, d_model)\n",
    "            )\n",
    "            self.head_dist = nn.Linear(d_model, n_cls)\n",
    "        else:\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.randn(1, self.patch_embed.num_patches + 1, d_model)\n",
    "            )\n",
    "\n",
    "        # transformer blocks\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, n_layers)]\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(d_model, n_heads, d_ff, dropout, dpr[i]) for i in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # output head\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, n_cls)\n",
    "        \n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=0.02)\n",
    "        trunc_normal_(self.cls_token, std=0.02)\n",
    "        if self.distilled:\n",
    "            trunc_normal_(self.dist_token, std=0.02)\n",
    "        self.pre_logits = nn.Identity()\n",
    "        self.decoder = DecoderLinear(19,16,192)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"pos_embed\", \"cls_token\", \"dist_token\"}\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=\"\"):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    def forward(self, im, return_features=False):\n",
    "        #print(\"ViT -> im shape\", im.shape)\n",
    "        B, T, H, W = im.shape\n",
    "        PS = self.patch_size\n",
    "\n",
    "        x = self.patch_embed(im)\n",
    "        #print(\"ViT -> after patch embed operation\", x.shape)\n",
    "        cls_tokens = self.cls_token.expand(B, -1,-1)\n",
    "        #print(\"ViT -> cls tokens\", cls_tokens.shape)\n",
    "        if self.distilled:\n",
    "            dist_tokens = self.dist_token.expand(B, -1, -1)\n",
    "            x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        pos_embed = self.pos_embed\n",
    "        #print(\"ViT -> pos_embed\", pos_embed.shape)\n",
    "        num_extra_tokens = 1 + self.distilled\n",
    "        if x.shape[1] != pos_embed.shape[1]:\n",
    "            pos_embed = resize_pos_embed(\n",
    "                pos_embed,\n",
    "                self.patch_embed.grid_size,\n",
    "                (H // PS, W // PS),\n",
    "                num_extra_tokens,\n",
    "            )\n",
    "        x = x + pos_embed\n",
    "        #print(\"ViT -> after pos embedding\", x.shape)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        #x = self.norm(x)\n",
    "        \n",
    "        \"\"\"x = x[:, 1:]\n",
    "        \n",
    "        masks = self.decoder(x, (H, W))\n",
    "        \n",
    "        masks = F.interpolate(masks, size=(H, W), mode=\"bilinear\")\n",
    "        masks = unpadding(masks, (768, 768))\"\"\"\n",
    "\n",
    "        #print(\"ViT Last step of encoder\", x.shape)\n",
    "        return x\n",
    "        #return masks\n",
    "\n",
    "    def get_attention_map(self, im, layer_id):\n",
    "        if layer_id >= self.n_layers or layer_id < 0:\n",
    "            raise ValueError(\n",
    "                f\"Provided layer_id: {layer_id} is not valid. 0 <= {layer_id} < {self.n_layers}.\"\n",
    "            )\n",
    "        B, _, H, W = im.shape\n",
    "        PS = self.patch_size\n",
    "\n",
    "        x = self.patch_embed(im)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        if self.distilled:\n",
    "            dist_tokens = self.dist_token.expand(B, -1, -1)\n",
    "            x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        pos_embed = self.pos_embed\n",
    "        num_extra_tokens = 1 + self.distilled\n",
    "        if x.shape[1] != pos_embed.shape[1]:\n",
    "            pos_embed = resize_pos_embed(\n",
    "                pos_embed,\n",
    "                self.patch_embed.grid_size,\n",
    "                (H // PS, W // PS),\n",
    "                num_extra_tokens,\n",
    "            )\n",
    "        x = x + pos_embed\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < layer_id:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                return blk(x, return_attention=True)\n",
    "            \n",
    "class DecoderLinear(nn.Module):\n",
    "    def __init__(self, n_cls, patch_size, d_encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_encoder = d_encoder\n",
    "        self.patch_size = patch_size\n",
    "        self.n_cls = n_cls\n",
    "\n",
    "        self.head = nn.Linear(self.d_encoder, n_cls)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return set()\n",
    "\n",
    "    def forward(self, x, im_size):\n",
    "        H, W = im_size\n",
    "        GS = H // self.patch_size\n",
    "        x = self.head(x)\n",
    "        x = rearrange(x, \"b (h w) c -> b c h w\", h=GS)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c11f8226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vit(nn.Module):\n",
    "    def __init__(self,image_size,patch_size,n_layers,d_model,d_ff,\n",
    "        n_heads,\n",
    "        n_cls,\n",
    "        dropout=0.1,\n",
    "        drop_path_rate=0.0,\n",
    "        distilled=False,\n",
    "        channels=3):\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_layers =  n_layers\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_cls = n_cls\n",
    "        self.d_ff = d_ff\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vit_encoder = VisionTransformer(image_size, patch_size,n_layers,d_model,n_cls,n_heads,d_ff)\n",
    "                \n",
    "        \n",
    "    def forward(self,images):\n",
    "        \n",
    "        num_seqs = images.shape[1]\n",
    "                \n",
    "        images = rearrange(images, \"b t c h w -> t b c h w\")\n",
    "        \n",
    "        vit_transformer = []\n",
    "        for i in range(num_seqs):\n",
    "            x=self.vit_encoder(images[i])   #-> images[i]-> B*C*H*W\n",
    "            vit_transformer.append(x)\n",
    "            \n",
    "        return torch.stack(vit_transformer)\n",
    "            \n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self,n_layers,d_model,d_ff,\n",
    "        n_heads,\n",
    "        dropout=0.1,\n",
    "        drop_path_rate=0.0,\n",
    "        distilled=False,\n",
    "        channels=3):\n",
    "        \n",
    "        self.n_layers =  n_layers\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        super().__init__()\n",
    "        \n",
    "        self.temporal_token = nn.Parameter(torch.randn(1,4,1,d_model))\n",
    "        \n",
    "        dpr = [x.item() for x in torch.linspace(0, 0.0, 12)]\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(d_model, n_heads, d_ff, dropout, dpr[i]) for i in range(n_layers)])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self,images, x):\n",
    "        \n",
    "        b,t,c,h,w = images.shape\n",
    "        \n",
    "        num_seqs = images.shape[1]\n",
    "        \n",
    "        #images=images.reshape(4,1,3,768,768)\n",
    "        \n",
    "        images = rearrange(images, \"b t c h w -> t b c h w\")\n",
    "        \n",
    "        vit_encoderoutput=[]\n",
    "        \n",
    "        predictor_embs=[]\n",
    "        \n",
    "        for i in range(num_seqs):\n",
    "            print(\"------------------------------------------------------------------------\")\n",
    "            print(\"For image\", i,\"in sequence\")\n",
    "            #x=self.vit_encoder(images[i])   #-> images[i]-> B*C*H*W\n",
    "            if(i==0):\n",
    "                vit_encoderoutput.append(x[i])\n",
    "                vit_embds = torch.stack(vit_encoderoutput)\n",
    "            else:\n",
    "                \n",
    "                vit_embds = rearrange(vit_embds,\"t b n d -> b t n d\")\n",
    "                #vit_embds = vit_embds.reshape(1,4,2305, 192)\n",
    "                self.temporal_token = nn.Parameter(torch.randn(1,i,1,d_model))\n",
    "\n",
    "                print(\"Temporal token\",self.temporal_token.shape)\n",
    "                #print(vit_embds.shape)\n",
    "\n",
    "                vit_embds = self.temporal_token + vit_embds\n",
    "\n",
    "                vit_embds = rearrange(vit_embds, \"b t n d -> b (t n) d\")\n",
    "                #vit_embds = vit_embds.reshape(1,4*2305,192)\n",
    "                print(\"Spatio-temporal Transformer input shape ->\", vit_embds.shape)\n",
    "                for blk in self.blocks:\n",
    "                    vit_embds = blk(vit_embds)\n",
    "\n",
    "                #Saving the output from the spatio-temporal transformer to be used for the corrector transformer\n",
    "                spatemp_embds = vit_embds\n",
    "\n",
    "                print(\"Spatial-transformer output embeddinngs\",spatemp_embds.shape)\n",
    "\n",
    "                spatemp_embds = rearrange(spatemp_embds , \"b (t n) d  -> b t n d\",t=i)\n",
    "\n",
    "                #Discarding one embedding\n",
    "\n",
    "                spatemp_embds = spatemp_embds[:,:1]\n",
    "                \n",
    "                predictor_embs.append(spatemp_embds)\n",
    "                \n",
    "                vit_encoderoutput.append(x[i])\n",
    "                vit_embds = torch.stack(vit_encoderoutput)\n",
    "                \n",
    "        return predictor_embs\n",
    "    \n",
    "    \n",
    "    \n",
    "class Corrector(nn.Module):\n",
    "    def __init__(self,n_layers,d_model,d_ff,\n",
    "        n_heads,\n",
    "        dropout=0.1,\n",
    "        drop_path_rate=0.0,\n",
    "        distilled=False,\n",
    "        channels=3):\n",
    "        \n",
    "        self.n_layers =  n_layers\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_ff = d_ff\n",
    "        super().__init__()\n",
    "                \n",
    "        dpr = [x.item() for x in torch.linspace(0, 0.0, 12)]\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(d_model, n_heads, d_ff, dropout, dpr[i]) for i in range(n_layers)])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self,images, x, predictor_embds):\n",
    "\n",
    "        b,t,c,h,w = images.shape\n",
    "        \n",
    "        num_seqs = images.shape[1]\n",
    "        \n",
    "        #images=images.reshape(4,1,3,768,768)\n",
    "        \n",
    "        images = rearrange(images, \"b t c h w -> t b c h w\")\n",
    "                        \n",
    "        corrector_embds = []\n",
    "        \n",
    "        for i in range(num_seqs-1):\n",
    "            print(\"------------------------------------------------------------------------\")\n",
    "            print(\"For image\", i,\"in sequence\")\n",
    "            #x=self.vit_encoder(images[i])   #-> images[i]-> B*C*H*W\n",
    "\n",
    "            predictor_embds[i] = rearrange(predictor_embds[i], \"b t n d -> b (t n) d\")\n",
    "\n",
    "            #Concatenating with vit_encoder output of current time step \n",
    "\n",
    "            corrector_input = torch.cat([predictor_embds[i] , x[i+1]], dim=1)\n",
    "\n",
    "            print(\"Corrector transformer Input -> \", corrector_input.shape )\n",
    "\n",
    "            #Passing through corrector transformer\n",
    "            for blk in self.blocks:\n",
    "                corrector_input = blk(corrector_input)\n",
    "\n",
    "            #After passing through transformer\n",
    "            print(\"After passing through corrector transformer\", corrector_input.shape)\n",
    "\n",
    "            #After rearranging\n",
    "            corrector_output = rearrange(corrector_input, \" b (t n) d -> b t n d\", t=2)\n",
    "\n",
    "\n",
    "            #Discardng one output\n",
    "            corrector_output = corrector_output[:,:1]\n",
    "\n",
    "            print(\"After discarding pne embedding from corrector transformer\", corrector_output.shape)\n",
    "\n",
    "            #Combining batch & time\n",
    "            corrector_output = rearrange(corrector_output,\"b t n d -> (b t) n d\")                \n",
    "\n",
    "            corrector_output = self.norm(corrector_output)\n",
    "            \n",
    "            corrector_embds.append(corrector_output)\n",
    "\n",
    "        return corrector_embds\n",
    "\n",
    "                \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,image_size,patch_size,d_model,\n",
    "        n_cls,\n",
    "        distilled=False,\n",
    "        channels=3):\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.d_model = d_model\n",
    "        self.n_cls = n_cls\n",
    "        super().__init__()\n",
    "        \n",
    "        self.decoder = DecoderLinear(n_cls,patch_size,d_model)\n",
    "\n",
    "    def forward(self, vit_embs, corrector_embds):\n",
    "        h,w = self.image_size\n",
    "        num_seqs = len(vit_embs)\n",
    "        segmented_output=[]\n",
    "        for i in range(num_seqs):\n",
    "            print(\"------------------------------------------------------------------------\")\n",
    "            print(\"For image\", i,\"in sequence\")\n",
    "            if(i==0):\n",
    "                decoder_input = vit_embs[i][:, 1:]\n",
    "                decoder_output = self.decoder(decoder_input,(h,w))\n",
    "                masks = F.interpolate(decoder_output, size=self.image_size, mode=\"bilinear\")\n",
    "                segmented_output.append(masks)\n",
    "            else:\n",
    "                #Decoder\n",
    "                decoder_input = corrector_embds[i-1]\n",
    "                print(decoder_input.shape)\n",
    "                decoder_input = decoder_input[:, 1:]\n",
    "\n",
    "                decoder_output = self.decoder(decoder_input,(h,w))\n",
    "\n",
    "                masks = F.interpolate(decoder_output, size=self.image_size, mode=\"bilinear\")\n",
    "                segmented_output.append(masks)\n",
    "\n",
    "                print(\"Final decoder output\", masks.shape)\n",
    "                \n",
    "        return segmented_output\n",
    "                \n",
    "\n",
    "\n",
    "class ViViT(nn.Module):\n",
    "    def __init__(self,image_size,patch_size,n_layers,d_model,d_ff,\n",
    "        n_heads,\n",
    "        n_cls,\n",
    "        dropout=0.1,\n",
    "        drop_path_rate=0.0,\n",
    "        distilled=False,\n",
    "        channels=3):\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_layers =  n_layers\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_cls = n_cls\n",
    "        self.d_ff = d_ff\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vit_embs = Vit(image_size, patch_size, n_layers, d_model,n_cls,n_heads, d_ff)\n",
    "        self.predictor_embs = Predictor( n_layers, d_model, d_ff, n_heads)\n",
    "        self.corrector_embs = Corrector( n_layers, d_model, d_ff, n_heads)\n",
    "        self.decoder = Decoder(image_size, patch_size, d_model, d_ff, n_cls)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        vit_embds = self.vit_embs(x)\n",
    "        \n",
    "        predictor_embds = self.predictor_embs(x, vit_embds)\n",
    "        \n",
    "        corrector_embds = self.corrector_embs(x, vit_embds, predictor_embds)\n",
    "        \n",
    "        output = self.decoder(vit_embds,corrector_embds)\n",
    "        \n",
    "        return output, vit_embds, predictor_embds, corrector_embds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ebbfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (768,768)\n",
    "patch_size = 16\n",
    "num_images = 4\n",
    "n_layers = 12\n",
    "d_model = 192\n",
    "d_ff = 4*192\n",
    "n_heads = 3\n",
    "n_cls = 19\n",
    "#model = VideoTransformer(image_size, patch_size, num_images, n_layers, d_model, d_ff, n_heads, n_cls, d_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b849e347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "For image 0 in sequence\n",
      "------------------------------------------------------------------------\n",
      "For image 1 in sequence\n",
      "Temporal token torch.Size([1, 1, 1, 192])\n",
      "Spatio-temporal Transformer input shape -> torch.Size([1, 2305, 192])\n",
      "Spatial-transformer output embeddinngs torch.Size([1, 2305, 192])\n",
      "------------------------------------------------------------------------\n",
      "For image 2 in sequence\n",
      "Temporal token torch.Size([1, 2, 1, 192])\n",
      "Spatio-temporal Transformer input shape -> torch.Size([1, 4610, 192])\n",
      "Spatial-transformer output embeddinngs torch.Size([1, 4610, 192])\n",
      "------------------------------------------------------------------------\n",
      "For image 3 in sequence\n",
      "Temporal token torch.Size([1, 3, 1, 192])\n",
      "Spatio-temporal Transformer input shape -> torch.Size([1, 6915, 192])\n",
      "Spatial-transformer output embeddinngs torch.Size([1, 6915, 192])\n",
      "------------------------------------------------------------------------\n",
      "For image 0 in sequence\n",
      "Corrector transformer Input ->  torch.Size([1, 4610, 192])\n",
      "After passing through corrector transformer torch.Size([1, 4610, 192])\n",
      "After discarding pne embedding from corrector transformer torch.Size([1, 1, 2305, 192])\n",
      "------------------------------------------------------------------------\n",
      "For image 1 in sequence\n",
      "Corrector transformer Input ->  torch.Size([1, 4610, 192])\n",
      "After passing through corrector transformer torch.Size([1, 4610, 192])\n",
      "After discarding pne embedding from corrector transformer torch.Size([1, 1, 2305, 192])\n",
      "------------------------------------------------------------------------\n",
      "For image 2 in sequence\n",
      "Corrector transformer Input ->  torch.Size([1, 4610, 192])\n",
      "After passing through corrector transformer torch.Size([1, 4610, 192])\n",
      "After discarding pne embedding from corrector transformer torch.Size([1, 1, 2305, 192])\n",
      "------------------------------------------------------------------------\n",
      "For image 0 in sequence\n",
      "------------------------------------------------------------------------\n",
      "For image 1 in sequence\n",
      "torch.Size([1, 2305, 192])\n",
      "Final decoder output torch.Size([1, 768, 768, 768])\n",
      "------------------------------------------------------------------------\n",
      "For image 2 in sequence\n",
      "torch.Size([1, 2305, 192])\n",
      "Final decoder output torch.Size([1, 768, 768, 768])\n",
      "------------------------------------------------------------------------\n",
      "For image 3 in sequence\n",
      "torch.Size([1, 2305, 192])\n",
      "Final decoder output torch.Size([1, 768, 768, 768])\n"
     ]
    }
   ],
   "source": [
    "images = torch.rand(1,4,3,768,768)\n",
    "model4 = ViViT(image_size, patch_size, n_layers, d_model, d_ff, n_heads, n_cls)\n",
    "x4 = model4(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bcd2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    vit, \n",
    "    corrector,\n",
    "    model,\n",
    "    data_loader,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    epoch,\n",
    "    amp_autocast,\n",
    "    loss_scaler,\n",
    "):\n",
    "    criterion1 = torch.nn.CrossEntropyLoss(ignore_index=IGNORE_LABEL)\n",
    "    criterion2 = torch.nn.MSELoss(ignore_index=IGNORE_LABEL)\n",
    "    logger = MetricLogger(delimiter=\"  \")\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "    print_freq = 100\n",
    "\n",
    "    model.train()\n",
    "    data_loader.set_epoch(epoch)\n",
    "    num_updates = epoch * len(data_loader)\n",
    "    for batch in logger.log_every(data_loader, print_freq, header):\n",
    "        im = batch[\"im\"].to(ptu.device)\n",
    "        seg_gt = batch[\"segmentation\"].long().to(ptu.device)\n",
    "\n",
    "        with amp_autocast():\n",
    "            output, vit_embds, predictor_embds, corrector_embds = model.forward(im)\n",
    "            loss1 = criterion2(corrector_embds, vit_embds)\n",
    "\n",
    "            loss2 = criterion1(output,seg_gt)\n",
    "            final_loss = loss1+loss2\n",
    "\n",
    "        loss_value = final_loss\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value), force=True)\n",
    "        optimizer.zero_grad()\n",
    "        if loss_scaler is not None:\n",
    "            loss_scaler(\n",
    "                loss,\n",
    "                optimizer,\n",
    "                parameters=model.parameters(),\n",
    "            )\n",
    "        else:\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        num_updates += 1\n",
    "        lr_scheduler.step_update(num_updates=num_updates)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        logger.update(\n",
    "            loss=loss.item(),\n",
    "            learning_rate=optimizer.param_groups[0][\"lr\"],\n",
    "        )\n",
    "\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07d48f43",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'segm_video'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistributedDataParallel \u001b[38;5;28;01mas\u001b[39;00m DDP\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfactory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msegm_video\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributed\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msegm_video\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mptu\u001b[39;00m\n",
      "File \u001b[0;32m~/segmenter/segm_video/model/factory.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_model\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision_transformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _create_vision_transformer\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msegm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisionTransformer\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msegm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m checkpoint_filter_fn\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msegm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecoderLinear\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'segm_video'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import click\n",
    "import os\n",
    "import argparse\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from factory import load_model\n",
    "from segm_video.utils import distributed\n",
    "import segm_video.utils.torch as ptu\n",
    "from segm_video import config\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from factory import create_segmenter\n",
    "from segm_video.optim.factory import create_optimizer, create_scheduler\n",
    "from segm_video.data.factory import create_dataset\n",
    "from segm_video.model.utils import num_params\n",
    "\n",
    "from timm.utils import NativeScaler\n",
    "from contextlib import suppress\n",
    "\n",
    "from segm_video.utils.distributed import sync_model\n",
    "from segm_video.engine import train_one_epoch, evaluate\n",
    "\n",
    "import wandb\n",
    "%env DATASET = \"/home/user/siddiquia0/dataset\"\n",
    "\n",
    "weight_decay = 0.0\n",
    "scheduler = \"polynomial\"\n",
    "optimizer = \"sgd\"\n",
    "resume = False\n",
    "eval_freq=None\n",
    "epochs=None\n",
    "normalization = None\n",
    "amp = False\n",
    "learning_rate =None\n",
    "log_dir = \"vivit\"\n",
    "dataset = \"synpick\"\n",
    "decoder = \"linear\" \n",
    "backbone = \"vit_tiny_patch16_384\"\n",
    "batch_size = None\n",
    "# start distributed mode\n",
    "ptu.set_gpu_mode(True)\n",
    "#distributed.init_process()\n",
    "\n",
    "# set up configuration\n",
    "cfg = config.load_config()\n",
    "model_cfg = cfg[\"model\"][backbone]\n",
    "dataset_cfg = cfg[\"dataset\"][dataset]\n",
    "if \"mask_transformer\" in decoder:\n",
    "    decoder_cfg = cfg[\"decoder\"][\"mask_transformer\"]\n",
    "else:\n",
    "    decoder_cfg = cfg[\"decoder\"][decoder]\n",
    "\n",
    "# model config\n",
    "im_size = dataset_cfg[\"im_size\"]\n",
    "crop_size = dataset_cfg.get(\"crop_size\", im_size)\n",
    "window_size = dataset_cfg.get(\"window_size\", im_size)\n",
    "window_stride = dataset_cfg.get(\"window_stride\", im_size)\n",
    "\n",
    "model_cfg[\"image_size\"] = (crop_size, crop_size)\n",
    "model_cfg[\"backbone\"] = backbone\n",
    "model_cfg[\"dropout\"] = 0.0\n",
    "model_cfg[\"drop_path_rate\"] = 0.1\n",
    "decoder_cfg[\"name\"] = decoder\n",
    "model_cfg[\"decoder\"] = decoder_cfg\n",
    "\n",
    "# dataset config\n",
    "world_batch_size = dataset_cfg[\"batch_size\"]\n",
    "num_epochs = dataset_cfg[\"epochs\"]\n",
    "lr = dataset_cfg[\"learning_rate\"]\n",
    "if batch_size:\n",
    "    world_batch_size = batch_size\n",
    "if epochs:\n",
    "    num_epochs = epochs\n",
    "if learning_rate:\n",
    "    lr = learning_rate\n",
    "if eval_freq is None:\n",
    "    eval_freq = dataset_cfg.get(\"eval_freq\", 1)\n",
    "\n",
    "if normalization:\n",
    "    model_cfg[\"normalization\"] = normalization\n",
    "\n",
    "# experiment config\n",
    "batch_size = world_batch_size // ptu.world_size\n",
    "variant = dict(\n",
    "    world_batch_size=world_batch_size,\n",
    "    version=\"normal\",\n",
    "    resume=resume,\n",
    "    dataset_kwargs=dict(\n",
    "        dataset=dataset,\n",
    "        image_size=im_size,\n",
    "        crop_size=crop_size,\n",
    "        batch_size=batch_size,\n",
    "        normalization=model_cfg[\"normalization\"],\n",
    "        split=\"train\",\n",
    "        num_workers=10,\n",
    "    ),\n",
    "    algorithm_kwargs=dict(\n",
    "        batch_size=batch_size,\n",
    "        start_epoch=0,\n",
    "        num_epochs=num_epochs,\n",
    "        eval_freq=eval_freq,\n",
    "    ),\n",
    "    optimizer_kwargs=dict(\n",
    "        opt=optimizer,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        momentum=0.9,\n",
    "        clip_grad=None,\n",
    "        sched=scheduler,\n",
    "        epochs=num_epochs,\n",
    "        min_lr=1e-5,\n",
    "        poly_power=0.9,\n",
    "        poly_step_size=1,\n",
    "    ),\n",
    "    net_kwargs=model_cfg,\n",
    "    amp=amp,\n",
    "    log_dir=log_dir,\n",
    "    inference_kwargs=dict(\n",
    "        im_size=im_size,\n",
    "        window_size=window_size,\n",
    "        window_stride=window_stride,\n",
    "    ),\n",
    ")\n",
    "#WANDB INITIALISATION\n",
    "wandb.init(project=\"AIS-thesis\", entity=\"aysha_athar\")\n",
    "\n",
    "log_dir = Path(log_dir)\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "checkpoint_path = log_dir / \"checkpoint.pth\"\n",
    "\n",
    "# dataset\n",
    "dataset_kwargs = variant[\"dataset_kwargs\"]\n",
    "\n",
    "train_loader = create_dataset(dataset_kwargs)\n",
    "val_kwargs = dataset_kwargs.copy()\n",
    "val_kwargs[\"split\"] = \"val\"\n",
    "val_kwargs[\"batch_size\"] = 1\n",
    "val_kwargs[\"crop\"] = False\n",
    "val_loader = create_dataset(val_kwargs)\n",
    "n_cls = train_loader.unwrapped.n_cls\n",
    "\n",
    "# model\n",
    "net_kwargs = variant[\"net_kwargs\"]\n",
    "net_kwargs[\"n_cls\"] = n_cls\n",
    "\n",
    "model = model4\n",
    "model.to(ptu.device)\n",
    "\n",
    "#model, _ = load_model(\"seg_ade20k_tiny/checkpoint.pth\")\n",
    "\n",
    "\n",
    "#model.decoder.mask_norm = nn.LayerNorm(n_cls)\n",
    "#model.decoder.cls_emb = nn.Parameter(torch.randn(1, n_cls, 192))\n",
    "#model.n_cls=19\n",
    "#model.to('cuda')\n",
    "print(model) \n",
    "\n",
    "\"\"\"for name, param in model.named_parameters():\n",
    "    param.requires_grad=False\n",
    "    if(name==\"decoder.mask_norm.weight\" or name==\"decoder.mask_norm.bias\" or name==\"decoder.cls_emb\" ):\n",
    "        param.requires_grad=True\n",
    "\n",
    "for name,param in model.named_parameters():\n",
    "    if(param.requires_grad==True):\n",
    "        print(name)\n",
    "        print(\"ok\")\"\"\"\n",
    "\n",
    "# optimizer\n",
    "optimizer_kwargs = variant[\"optimizer_kwargs\"]\n",
    "optimizer_kwargs[\"iter_max\"] = len(train_loader) * optimizer_kwargs[\"epochs\"]\n",
    "optimizer_kwargs[\"iter_warmup\"] = 0.0\n",
    "opt_args = argparse.Namespace()\n",
    "opt_vars = vars(opt_args)\n",
    "for k, v in optimizer_kwargs.items():\n",
    "    opt_vars[k] = v\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001,momentum=0.9,nesterov=True)\n",
    "lr_scheduler = create_scheduler(opt_args, optimizer)\n",
    "num_iterations = 0\n",
    "amp_autocast = suppress\n",
    "loss_scaler = None\n",
    "if amp:\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    loss_scaler = NativeScaler()\n",
    "\n",
    "# resume\n",
    "if resume and checkpoint_path.exists():\n",
    "    print(f\"Resuming training from checkpoint: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if loss_scaler and \"loss_scaler\" in checkpoint:\n",
    "        loss_scaler.load_state_dict(checkpoint[\"loss_scaler\"])\n",
    "    lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
    "    variant[\"algorithm_kwargs\"][\"start_epoch\"] = checkpoint[\"epoch\"] + 1\n",
    "else:\n",
    "    sync_model(log_dir, model)\n",
    "\n",
    "\"\"\"if ptu.distributed:\n",
    "    model = DDP(model, device_ids=[ptu.device], find_unused_parameters=True)\"\"\"\n",
    "\n",
    "# save config\n",
    "variant_str = yaml.dump(variant)\n",
    "#print(f\"Configuration:\\n{variant_str}\")\n",
    "variant[\"net_kwargs\"] = net_kwargs\n",
    "variant[\"dataset_kwargs\"] = dataset_kwargs\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "with open(log_dir / \"variant.yml\", \"w\") as f:\n",
    "    f.write(variant_str)\n",
    "\n",
    "# train\n",
    "start_epoch = variant[\"algorithm_kwargs\"][\"start_epoch\"]\n",
    "num_epochs = variant[\"algorithm_kwargs\"][\"num_epochs\"]\n",
    "eval_freq = variant[\"algorithm_kwargs\"][\"eval_freq\"]\n",
    "\n",
    "model_without_ddp = model\n",
    "if hasattr(model, \"module\"):\n",
    "    model_without_ddp = model.module\n",
    "\n",
    "val_seg_gt = val_loader.dataset.get_gt_seg_maps()\n",
    "\n",
    "print(f\"Train dataset length: {len(train_loader.dataset)}\")\n",
    "print(f\"Val dataset length: {len(val_loader.dataset)}\")\n",
    "#print(f\"Encoder parameters: {num_params(model_without_ddp.encoder)}\")\n",
    "print(f\"Decoder parameters: {num_params(model_without_ddp.decoder)}\")\n",
    "filename_wandb=[]\n",
    "for i in range(0,5):\n",
    "    dict1=(next(iter(val_loader)))\n",
    "    filename_wandb.append(dict1[\"im_metas\"][0][\"ori_filename\"][0])\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    # train for one epoch\n",
    "    train_logger = train_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        lr_scheduler,\n",
    "        epoch,\n",
    "        amp_autocast,\n",
    "        loss_scaler,\n",
    "    )\n",
    "\n",
    "    # save checkpoint\n",
    "    if ptu.dist_rank == 0:\n",
    "        snapshot = dict(\n",
    "            model=model_without_ddp.state_dict(),\n",
    "            optimizer=optimizer.state_dict(),\n",
    "            n_cls=model_without_ddp.n_cls,\n",
    "            lr_scheduler=lr_scheduler.state_dict(),\n",
    "        )\n",
    "        if loss_scaler is not None:\n",
    "            snapshot[\"loss_scaler\"] = loss_scaler.state_dict()\n",
    "        snapshot[\"epoch\"] = epoch\n",
    "        torch.save(snapshot, checkpoint_path)\n",
    "\n",
    "    # evaluate\n",
    "    eval_epoch = epoch % eval_freq == 0 or epoch == num_epochs - 1\n",
    "    if eval_epoch:\n",
    "        eval_logger = evaluate(\n",
    "            model,\n",
    "            val_loader,\n",
    "            val_seg_gt,\n",
    "            window_size,\n",
    "            window_stride,\n",
    "            amp_autocast,\n",
    "            filename_wandb,\n",
    "            epoch,\n",
    "        )\n",
    "        print(f\"Stats [{epoch}]:\", eval_logger, flush=True)\n",
    "        print(\"\")\n",
    "        \n",
    "\n",
    "    # log stats\n",
    "    if ptu.dist_rank == 0:\n",
    "        train_stats = {\n",
    "            k: meter.global_avg for k, meter in train_logger.meters.items()\n",
    "        }\n",
    "        val_stats = {}\n",
    "        if eval_epoch:\n",
    "            val_stats = {\n",
    "                k: meter.global_avg for k, meter in eval_logger.meters.items()\n",
    "            }\n",
    "            print(val_stats['mean_iou'])\n",
    "            print(type(val_stats))\n",
    "            wandb.log({\"Mean_iou\": val_stats['mean_iou']})\n",
    "        log_stats = {\n",
    "            **{f\"train_{k}\": v for k, v in train_stats.items()},\n",
    "            **{f\"val_{k}\": v for k, v in val_stats.items()},\n",
    "            \"epoch\": epoch,\n",
    "            \"num_updates\": (epoch + 1) * len(train_loader),\n",
    "        }\n",
    "\n",
    "        with open(log_dir / \"log.txt\", \"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "sys.exit(1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303cd1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
