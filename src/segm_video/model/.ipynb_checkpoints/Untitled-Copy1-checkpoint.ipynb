{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b26f1be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/siddiquia0/anaconda3/envs/proj/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.layers import DropPath\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import _load_weights\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from pathlib import Path\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from timm.models.layers import DropPath\n",
    "\n",
    "\n",
    "class FeedForward1(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout, out_dim=None):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        if out_dim is None:\n",
    "            out_dim = dim\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        head_dim = dim // heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.attn = None\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        return self\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, N, C = x.shape\n",
    "        print(x.shape)\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B, N, 3, self.heads, C // self.heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q, k, v = (\n",
    "            qkv[0],\n",
    "            qkv[1],\n",
    "            qkv[2],\n",
    "        )\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x, attn\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_dim, dropout, drop_path):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, heads, dropout)\n",
    "        self.mlp = FeedForward1(dim, mlp_dim, dropout)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        y, attn = self.attn(self.norm1(x), mask)\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        trunc_normal_(m.weight, std=0.02)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "\n",
    "def resize_pos_embed(posemb, grid_old_shape, grid_new_shape, num_extra_tokens):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    posemb_tok, posemb_grid = (\n",
    "        posemb[:, :num_extra_tokens],\n",
    "        posemb[0, num_extra_tokens:],\n",
    "    )\n",
    "    if grid_old_shape is None:\n",
    "        gs_old_h = int(math.sqrt(len(posemb_grid)))\n",
    "        gs_old_w = gs_old_h\n",
    "    else:\n",
    "        gs_old_h, gs_old_w = grid_old_shape\n",
    "\n",
    "    gs_h, gs_w = grid_new_shape\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old_h, gs_old_w, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=(gs_h, gs_w), mode=\"bilinear\")\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_h * gs_w, -1)\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, embed_dim, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        print(image_size)\n",
    "        if image_size[0] % patch_size != 0 or image_size[1] % patch_size != 0:\n",
    "            raise ValueError(\"image dimensions must be divisible by the patch size\")\n",
    "        self.grid_size = image_size[0] // patch_size, image_size[1] // patch_size\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.patch_size = patch_size\n",
    "        print(\"Patch embedding embed dim\", embed_dim)\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            channels, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, im):\n",
    "        #B, T, C, H, W = im.shape\n",
    "        B, C, H, W = im.shape\n",
    "        #print(\"efore rearragne\",im.shape)\n",
    "        #im = rearrange(im,\n",
    "                #\"b t c (h ph) (w pw) -> (b t) c (h ph) (w pw)\",\n",
    "                #ph=16,\n",
    "                #pw=16,\n",
    "            #)\n",
    "        print(\"Patch Embedding im shape\", im.shape)\n",
    "        x = self.proj(im).flatten(2).transpose(1, 2)\n",
    "        #x=rearrange(x, \"(b t) h w -> b t h w \",b=1,t=4)\n",
    "        \n",
    "        #x = self.proj(im)\n",
    "        #x=rearrange(x, \"b c h w -> b (h w) c \")\n",
    "        print(\"Patch embedding After proj operation x shape\", x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        n_layers,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        n_heads,\n",
    "        n_cls,\n",
    "        dropout=0.1,\n",
    "        drop_path_rate=0.0,\n",
    "        distilled=False,\n",
    "        channels=3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            image_size,\n",
    "            patch_size,\n",
    "            d_model,\n",
    "            channels,\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.n_cls = n_cls\n",
    "\n",
    "        # cls and pos tokens\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.distilled = distilled\n",
    "        if self.distilled:\n",
    "            self.dist_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.randn(1, self.patch_embed.num_patches + 2, d_model)\n",
    "            )\n",
    "            self.head_dist = nn.Linear(d_model, n_cls)\n",
    "        else:\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.randn(1, self.patch_embed.num_patches + 1, d_model)\n",
    "            )\n",
    "\n",
    "        # transformer blocks\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, n_layers)]\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(d_model, n_heads, d_ff, dropout, dpr[i]) for i in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # output head\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, n_cls)\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=0.02)\n",
    "        trunc_normal_(self.cls_token, std=0.02)\n",
    "        if self.distilled:\n",
    "            trunc_normal_(self.dist_token, std=0.02)\n",
    "        self.pre_logits = nn.Identity()\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"pos_embed\", \"cls_token\", \"dist_token\"}\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=\"\"):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "\n",
    "    def forward(self, im, return_features=False):\n",
    "        print(\"Vision transformer im shape\", im.shape)\n",
    "        B,T, H, W = im.shape\n",
    "        PS = self.patch_size\n",
    "\n",
    "        x = self.patch_embed(im)\n",
    "        print(\"ViT after patch embed operation\", x.shape)\n",
    "        cls_tokens = self.cls_token.expand(B, -1,-1)\n",
    "        print(\"ViT cls tokens\", cls_tokens.shape)\n",
    "        if self.distilled:\n",
    "            dist_tokens = self.dist_token.expand(B, -1, -1)\n",
    "            x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "            print(\"ViT after cat operatiom\", x.shape)\n",
    "\n",
    "        pos_embed = self.pos_embed\n",
    "        print(\"ViT pos_embed\", pos_embed.shape)\n",
    "        num_extra_tokens = 1 + self.distilled\n",
    "        if x.shape[1] != pos_embed.shape[1]:\n",
    "            pos_embed = resize_pos_embed(\n",
    "                pos_embed,\n",
    "                self.patch_embed.grid_size,\n",
    "                (H // PS, W // PS),\n",
    "                num_extra_tokens,\n",
    "            )\n",
    "        x = x + pos_embed\n",
    "        print(\"ViT after adding pos embedding\", x.shape)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        if return_features:\n",
    "            return x\n",
    "\n",
    "        \"\"\"if self.distilled:\n",
    "            x, x_dist = x[:, 0], x[:, 1]\n",
    "            x = self.head(x)\n",
    "            x_dist = self.head_dist(x_dist)\n",
    "            x = (x + x_dist) / 2\n",
    "        else:\n",
    "            x = x[:, 0]\n",
    "            x = self.head(x)\"\"\"\n",
    "        print(\"ViT Last step of encoder\", x.shape)\n",
    "        return x\n",
    "\n",
    "    def get_attention_map(self, im, layer_id):\n",
    "        if layer_id >= self.n_layers or layer_id < 0:\n",
    "            raise ValueError(\n",
    "                f\"Provided layer_id: {layer_id} is not valid. 0 <= {layer_id} < {self.n_layers}.\"\n",
    "            )\n",
    "        B, _, H, W = im.shape\n",
    "        PS = self.patch_size\n",
    "\n",
    "        x = self.patch_embed(im)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        if self.distilled:\n",
    "            dist_tokens = self.dist_token.expand(B, -1, -1)\n",
    "            x = torch.cat((cls_tokens, dist_tokens, x), dim=1)\n",
    "        else:\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        pos_embed = self.pos_embed\n",
    "        num_extra_tokens = 1 + self.distilled\n",
    "        if x.shape[1] != pos_embed.shape[1]:\n",
    "            pos_embed = resize_pos_embed(\n",
    "                pos_embed,\n",
    "                self.patch_embed.grid_size,\n",
    "                (H // PS, W // PS),\n",
    "                num_extra_tokens,\n",
    "            )\n",
    "        x = x + pos_embed\n",
    "\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < layer_id:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                return blk(x, return_attention=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fa65a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def pair(t):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    t: tuple[int] or int\n",
    "    \"\"\"\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class BaseClassificationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -----------\n",
    "    img_size: int\n",
    "        Size of the image\n",
    "    patch_size: int or tuple(int)\n",
    "        Size of the patch\n",
    "    in_channels: int\n",
    "        Number of channels in input image\n",
    "    pool: str\n",
    "        Feature pooling type, must be one of {``mean``, ``cls``}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size, patch_size, in_channels=3, pool=\"cls\"):\n",
    "        super(BaseClassificationModel, self).__init__()\n",
    "\n",
    "        img_height, img_width = pair(img_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert (\n",
    "            img_height % patch_height == 0 and img_width % patch_width == 0\n",
    "        ), \"Image dimensions must be divisible by the patch size.\"\n",
    "\n",
    "        num_patches = (img_height // patch_height) * (img_width // patch_width)\n",
    "        patch_dim = in_channels * patch_height * patch_width\n",
    "\n",
    "        self.patch_height = patch_height\n",
    "        self.patch_width = patch_width\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_dim = patch_dim\n",
    "\n",
    "        assert pool in {\n",
    "            \"cls\",\n",
    "            \"mean\",\n",
    "        }, \"Feature pooling type must be either cls (cls token) or mean (mean pooling)\"\n",
    "        self.pool = pool\n",
    "        \n",
    "class ViViTModel2(BaseClassificationModel):\n",
    "    \"\"\"\n",
    "    Model 2 implementation of: `ViViT: A Video Vision Transformer <https://arxiv.org/abs/2103.15691>`_\n",
    "    Parameters\n",
    "    -----------\n",
    "    img_size:int\n",
    "        Size of single frame/ image in video\n",
    "    in_channels:int\n",
    "        Number of channels\n",
    "    patch_size: int\n",
    "        Patch size\n",
    "    embedding_dim: int\n",
    "        Embedding dimension of a patch\n",
    "    num_frames:int\n",
    "        Number of seconds in each Video\n",
    "    depth:int\n",
    "        Number of encoder layers\n",
    "    num_heads:int\n",
    "        Number of attention heads\n",
    "    head_dim:int\n",
    "        Dimension of head\n",
    "    n_classes:int\n",
    "        Number of classes\n",
    "    mlp_dim: int\n",
    "        Dimension of hidden layer\n",
    "    pool: str\n",
    "        Pooling operation,must be one of {\"cls\",\"mean\"},default is \"cls\"\n",
    "    p_dropout:float\n",
    "        Dropout probability\n",
    "    attn_dropout:float\n",
    "        Dropout probability\n",
    "    drop_path_rate:float\n",
    "        Stochastic drop path rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size,\n",
    "        in_channels,\n",
    "        patch_size,\n",
    "        embedding_dim,\n",
    "        num_frames,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        head_dim,\n",
    "        n_classes,\n",
    "        mlp_dim=None,\n",
    "        pool=\"cls\",\n",
    "        p_dropout=0.0,\n",
    "        attn_dropout=0.0,\n",
    "        drop_path_rate=0.02,\n",
    "    ):\n",
    "        super(ViViTModel2, self).__init__(\n",
    "            img_size=img_size,\n",
    "            in_channels=in_channels,\n",
    "            patch_size=patch_size,\n",
    "            pool=pool,\n",
    "        )\n",
    "\n",
    "        patch_dim = in_channels * patch_size**2\n",
    "        self.patch_embedding = LinearVideoEmbedding(\n",
    "            embedding_dim=embedding_dim,\n",
    "            patch_height=patch_size,\n",
    "            patch_width=patch_size,\n",
    "            patch_dim=patch_dim,\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = PosEmbedding(\n",
    "            shape=[num_frames, self.num_patches + 1], dim=embedding_dim, drop=p_dropout\n",
    "        )\n",
    "\n",
    "        self.space_token = nn.Parameter(\n",
    "            torch.randn(1, 1, embedding_dim)\n",
    "        )  # this is similar to using cls token in vanilla vision transformer\n",
    "        self.spatial_transformer = VanillaEncoder(\n",
    "            embedding_dim=embedding_dim,\n",
    "            depth=depth,\n",
    "            num_heads=num_heads,\n",
    "            head_dim=head_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            p_dropout=p_dropout,\n",
    "            attn_dropout=attn_dropout,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "        )\n",
    "\n",
    "        self.time_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "        self.temporal_transformer = VanillaEncoder(\n",
    "            embedding_dim=embedding_dim,\n",
    "            depth=depth,\n",
    "            num_heads=num_heads,\n",
    "            head_dim=head_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            p_dropout=p_dropout,\n",
    "            attn_dropout=attn_dropout,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Beforeeee patch emedding\", x.shape)\n",
    "\n",
    "        x = self.patch_embedding(x)\n",
    "        print(\"Afterrrrrr patch emedding\", x.shape)\n",
    "\n",
    "        (\n",
    "            b,\n",
    "            t,\n",
    "            n,\n",
    "            d,\n",
    "        ) = x.shape  # shape of x will be number of videos,time,num_frames,embedding dim\n",
    "        print(\"After reshapig?\",x.shape)\n",
    "        print(\"space token\",self.space_token.shape)\n",
    "        \n",
    "        cls_space_tokens = repeat(self.space_token, \"() n d -> b t n d\", b=b, t=t)\n",
    "        print(\"xls_space_topen\",cls_space_tokens.shape)\n",
    "\n",
    "        x = nn.Parameter(torch.cat((cls_space_tokens, x), dim=2))\n",
    "        x = self.pos_embedding(x)\n",
    "\n",
    "        x = rearrange(x, \"b t n d -> (b t) n d\")\n",
    "        x = self.spatial_transformer(x)\n",
    "        print(\"after spaial shape of x\",x.shape)\n",
    "        x = rearrange(x[:, 0], \"(b t) ... -> b t ...\", b=b)\n",
    "        print(\"after rearrange\",x.shape)\n",
    "        print(\"temporal token\",self.time_token.shape)\n",
    "\n",
    "        cls_temporal_tokens = repeat(self.time_token, \"() n d -> b n d\", b=b)\n",
    "        print(\"cls_temporal\", cls_temporal_tokens.shape)\n",
    "        x = torch.cat((cls_temporal_tokens, x), dim=1)\n",
    "        print(\"after cat operation temporal\",x.shape)\n",
    "\n",
    "        x = self.temporal_transformer(x)\n",
    "        \n",
    "        print(\"after temporal\", x.shape)\n",
    "\n",
    "        x = x.mean(dim=1) if self.pool == \"mean\" else x[:, 0]\n",
    "        print(\"final shape\", x.shape)\n",
    "\n",
    "        #x = self.decoder(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f185546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8bb23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_dim: int\n",
    "        Dimension of the embedding\n",
    "    depth: int\n",
    "        Number of self-attention layers\n",
    "    num_heads: int\n",
    "        Number of the attention heads\n",
    "    head_dim: int\n",
    "        Dimension of each head\n",
    "    mlp_dim: int\n",
    "        Dimension of the hidden layer in the feed-forward layer\n",
    "    p_dropout: float\n",
    "        Dropout Probability\n",
    "    attn_dropout: float\n",
    "        Dropout Probability\n",
    "    drop_path_rate: float\n",
    "        Stochastic drop path rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        head_dim,\n",
    "        mlp_dim,\n",
    "        p_dropout=0.0,\n",
    "        attn_dropout=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        drop_path_mode=\"batch\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.encoder.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        PreNorm(\n",
    "                            dim=embedding_dim,\n",
    "                            fn=VanillaSelfAttention(\n",
    "                                dim=embedding_dim,\n",
    "                                num_heads=num_heads,\n",
    "                                head_dim=head_dim,\n",
    "                                p_dropout=attn_dropout,\n",
    "                            ),\n",
    "                        ),\n",
    "                        PreNorm(\n",
    "                            dim=embedding_dim,\n",
    "                            fn=FeedForward(\n",
    "                                dim=embedding_dim,\n",
    "                                hidden_dim=mlp_dim,\n",
    "                                p_dropout=p_dropout,\n",
    "                            ),\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        self.drop_path = (\n",
    "            StochasticDepth(p=drop_path_rate, mode=drop_path_mode)\n",
    "            if drop_path_rate > 0.0\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            Returns output tensor\n",
    "        \"\"\"\n",
    "        for attn, ff in self.encoder:\n",
    "            x = attn(x) + x\n",
    "            x = self.drop_path(ff(x)) + x\n",
    "\n",
    "        return x\n",
    "class PosEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Generalised Positional Embedding class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, shape, dim, drop=None, sinusoidal=False, std=0.02):\n",
    "        super(PosEmbedding, self).__init__()\n",
    "        print(\"shape\",shape)\n",
    "        if not sinusoidal:\n",
    "            if isinstance(shape, int):\n",
    "                shape = [1, shape, dim]\n",
    "            else:\n",
    "                shape = [1] + list(shape) + [dim]\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(shape))\n",
    "            print(\"pos embed shape\",self.pos_embed.shape)\n",
    "\n",
    "        else:\n",
    "            pe = torch.FloatTensor(\n",
    "                [\n",
    "                    [p / (10000 ** (2 * (i // 2) / dim)) for i in range(dim)]\n",
    "                    for p in range(shape)\n",
    "                ]\n",
    "            )\n",
    "            pe[:, 0::2] = torch.sin(pe[:, 0::2])\n",
    "            pe[:, 1::2] = torch.cos(pe[:, 1::2])\n",
    "            self.pos_embed = pe\n",
    "            self.pos_embed.requires_grad = False\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=std)\n",
    "        self.pos_drop = nn.Dropout(drop) if drop is not None else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"before pos embed\", x.shape)\n",
    "        x = x + self.pos_embed\n",
    "        return self.pos_drop(x)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "class LinearVideoEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -----------\n",
    "    embedding_dim: int\n",
    "        Dimension of the resultant embedding\n",
    "    patch_height: int\n",
    "        Height of the patch\n",
    "    patch_width: int\n",
    "        Width of the patch\n",
    "    patch_dim: int\n",
    "        patch_dimension\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        patch_height,\n",
    "        patch_width,\n",
    "        patch_dim,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.patch_embedding = nn.Sequential(\n",
    "            Rearrange(\n",
    "                \"b t c (h ph) (w pw) -> b t (h w) (ph pw c)\",\n",
    "                ph=patch_height,\n",
    "                pw=patch_width,\n",
    "            ),\n",
    "            nn.Linear(patch_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -----------\n",
    "        x: torch.Tensor\n",
    "            Input tensor\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            Returns patch embeddings of size `embedding_dim`\n",
    "        \"\"\"\n",
    "        print(\"eforeeeeeeeeeeeeee\",x.shape)\n",
    "        return self.patch_embedding(x)\n",
    "    \n",
    "class PreNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim: int\n",
    "        Dimension of the embedding\n",
    "    fn:nn.Module\n",
    "        Attention class\n",
    "    context_dim: int\n",
    "        Dimension of the context array used in cross attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, fn, context_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.context_norm = (\n",
    "            nn.LayerNorm(context_dim) if context_dim is not None else None\n",
    "        )\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if \"context\" in kwargs.keys() and kwargs[\"context\"] is not None:\n",
    "            normed_context = self.context_norm(kwargs[\"context\"])\n",
    "            kwargs.update(context=normed_context)\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class VanillaSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla O(:math:`n^2`) Self attention introduced in `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>`_\n",
    "    Parameters\n",
    "    -----------\n",
    "    dim: int\n",
    "        Dimension of the embedding\n",
    "    num_heads: int\n",
    "        Number of the attention heads\n",
    "    head_dim: int\n",
    "        Dimension of each head\n",
    "    p_dropout: float\n",
    "        Dropout Probability\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads=8, head_dim=64, p_dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        inner_dim = head_dim * num_heads\n",
    "        project_out = not (num_heads == 1 and head_dim == dim)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = (\n",
    "            nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(p_dropout))\n",
    "            if project_out\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            Input tensor\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            Returns output tensor by applying self-attention on input tensor\n",
    "        \"\"\"\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=self.num_heads), qkv\n",
    "        )\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        #out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "\n",
    "        return self.to_out(out)\n",
    "    \n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim: int\n",
    "        Dimension of the input tensor\n",
    "    hidden_dim: int, optional\n",
    "        Dimension of hidden layer\n",
    "    out_dim: int, optional\n",
    "        Dimension of the output tensor\n",
    "    p_dropout: float\n",
    "        Dropout probability, default=0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, hidden_dim=None, out_dim=None, p_dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        out_dim = out_dim if out_dim is not None else dim\n",
    "        hidden_dim = hidden_dim if hidden_dim is not None else dim\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p_dropout),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "            nn.Dropout(p_dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            Input tensor\n",
    "        Returns\n",
    "        ----------\n",
    "        torch.Tensor\n",
    "            Returns output tensor by performing linear operations and activation on input tensor\n",
    "        \"\"\"\n",
    "\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40298490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape [4, 2305]\n",
      "pos embed shape torch.Size([1, 4, 2305, 192])\n",
      "Beforeeee patch emedding torch.Size([1, 4, 3, 768, 768])\n",
      "eforeeeeeeeeeeeeee torch.Size([1, 4, 3, 768, 768])\n",
      "Afterrrrrr patch emedding torch.Size([1, 4, 2304, 192])\n",
      "After reshapig? torch.Size([1, 4, 2304, 192])\n",
      "space token torch.Size([1, 1, 192])\n",
      "xls_space_topen torch.Size([1, 4, 1, 192])\n",
      "before pos embed torch.Size([1, 4, 2305, 192])\n"
     ]
    },
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"b n (h d) -> b h n d\".\n Input tensor shape: torch.Size([4, 4, 2305, 3]). Additional info: {'h': 1}.\n Expected 3 dimensions, got 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/proj/lib/python3.10/site-packages/einops/einops.py:413\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    412\u001b[0m     recipe \u001b[38;5;241m=\u001b[39m _prepare_transformation_recipe(pattern, reduction, axes_lengths\u001b[38;5;241m=\u001b[39mhashable_axes_lengths)\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_recipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EinopsError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/proj/lib/python3.10/site-packages/einops/einops.py:236\u001b[0m, in \u001b[0;36m_apply_recipe\u001b[0;34m(recipe, tensor, reduction_type)\u001b[0m\n\u001b[1;32m    234\u001b[0m backend \u001b[38;5;241m=\u001b[39m get_backend(tensor)\n\u001b[1;32m    235\u001b[0m init_shapes, reduced_axes, axes_reordering, added_axes, final_shapes \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 236\u001b[0m     \u001b[43m_reconstruct_from_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m tensor \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mreshape(tensor, init_shapes)\n",
      "File \u001b[0;32m~/anaconda3/envs/proj/lib/python3.10/site-packages/einops/einops.py:166\u001b[0m, in \u001b[0;36m_reconstruct_from_shape_uncached\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_composite_axes):\n\u001b[0;32m--> 166\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_composite_axes), \u001b[38;5;28mlen\u001b[39m(shape)))\n\u001b[1;32m    168\u001b[0m ellipsis_shape: List[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mEinopsError\u001b[0m: Expected 3 dimensions, got 4",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 18\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Example data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m ViViTModel2(\n\u001b[1;32m      8\u001b[0m         img_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m768\u001b[39m,\n\u001b[1;32m      9\u001b[0m         patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         head_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     17\u001b[0m     )\n\u001b[0;32m---> 18\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/proj/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [2], line 169\u001b[0m, in \u001b[0;36mViViTModel2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    166\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding(x)\n\u001b[1;32m    168\u001b[0m x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb t n d -> (b t) n d\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 169\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspatial_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter spaial shape of x\u001b[39m\u001b[38;5;124m\"\u001b[39m,x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    171\u001b[0m x \u001b[38;5;241m=\u001b[39m rearrange(x[:, \u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(b t) ... -> b t ...\u001b[39m\u001b[38;5;124m\"\u001b[39m, b\u001b[38;5;241m=\u001b[39mb)\n",
      "File \u001b[0;32m~/anaconda3/envs/proj/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [3], line 79\u001b[0m, in \u001b[0;36mVanillaEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    Returns output tensor\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attn, ff \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder:\n\u001b[0;32m---> 79\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m     80\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(ff(x)) \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/proj/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [3], line 193\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     normed_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_norm(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    192\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(context\u001b[38;5;241m=\u001b[39mnormed_context)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/proj/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [3], line 240\u001b[0m, in \u001b[0;36mVanillaSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    Returns output tensor by applying self-attention on input tensor\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_qkv(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 240\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m t: rearrange(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb n (h d) -> b h n d\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads), qkv\n\u001b[1;32m    242\u001b[0m )\n\u001b[1;32m    244\u001b[0m dots \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[1;32m    246\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattend(dots)\n",
      "Cell \u001b[0;32mIn [3], line 241\u001b[0m, in \u001b[0;36mVanillaSelfAttention.forward.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    Returns output tensor by applying self-attention on input tensor\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_qkv(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    240\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb n (h d) -> b h n d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m, qkv\n\u001b[1;32m    242\u001b[0m )\n\u001b[1;32m    244\u001b[0m dots \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n\u001b[1;32m    246\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattend(dots)\n",
      "File \u001b[0;32m~/anaconda3/envs/proj/lib/python3.10/site-packages/einops/einops.py:484\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRearrange can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be applied to an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    483\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m get_backend(tensor[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstack_on_zeroth_dimension(tensor)\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrearrange\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/proj/lib/python3.10/site-packages/einops/einops.py:421\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    419\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Input is list. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    420\u001b[0m message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdditional info: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(axes_lengths)\n\u001b[0;32m--> 421\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(message \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e))\n",
      "\u001b[0;31mEinopsError\u001b[0m:  Error while processing rearrange-reduction pattern \"b n (h d) -> b h n d\".\n Input tensor shape: torch.Size([4, 4, 2305, 3]). Additional info: {'h': 1}.\n Expected 3 dimensions, got 4"
     ]
    }
   ],
   "source": [
    "from torchvision.ops import StochasticDepth\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "images = torch.randn(1, 4 , 3, 768, 768)\n",
    "\n",
    "# Example data\n",
    "model = ViViTModel2(\n",
    "        img_size=768,\n",
    "        patch_size=16,\n",
    "        num_frames = 4,\n",
    "        in_channels=3,\n",
    "        n_classes=10,\n",
    "        embedding_dim=192,\n",
    "        depth=2,\n",
    "        num_heads =1,\n",
    "        head_dim=3,\n",
    "    )\n",
    "logits = model(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2803428",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        num_frames,\n",
    "        n_layers,\n",
    "        d_model,\n",
    "        d_ff,\n",
    "        n_heads,\n",
    "        n_cls,\n",
    "        dropout=0.1,\n",
    "        drop_path_rate=0.0,\n",
    "        distilled=False,\n",
    "        channels=3\n",
    "    ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078275ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images=torch.rand(4,3,768,768)\n",
    "model1=VisionTransformer(image_size=(768,768), patch_size=16,n_layers=12,d_model=192,n_cls=10,n_heads=3,d_ff=4*192)\n",
    "logits1=model1(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a502d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [torch.rand(8,2048,192),torch.rand(8,2048,192)]\n",
    "data1 = torch.cat(data,dim=0)\n",
    "data1=data1.reshape(8,2,2048,192)\n",
    "print(data1[2].shape)\n",
    "#data1=rearrange(data1, 's b n w h -> b s n w h')\n",
    "#data1=data1.permute(1,0,2,3)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10061dd9",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c450d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 768, 768])\n",
      "(768, 768)\n",
      "Patch embedding embed dim 192\n",
      "Vision transformer im shape torch.Size([1, 3, 768, 768])\n",
      "Patch Embedding im shape torch.Size([1, 3, 768, 768])\n",
      "Patch embedding After proj operation x shape torch.Size([1, 2304, 192])\n",
      "ViT after patch embed operation torch.Size([1, 2304, 192])\n",
      "ViT cls tokens torch.Size([1, 1, 192])\n",
      "ViT after cat operatiom torch.Size([1, 2305, 192])\n",
      "ViT pos_embed torch.Size([1, 2305, 192])\n",
      "ViT after adding pos embedding torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "ViT Last step of encoder torch.Size([1, 2305, 192])\n",
      "Vision transformer im shape torch.Size([1, 3, 768, 768])\n",
      "Patch Embedding im shape torch.Size([1, 3, 768, 768])\n",
      "Patch embedding After proj operation x shape torch.Size([1, 2304, 192])\n",
      "ViT after patch embed operation torch.Size([1, 2304, 192])\n",
      "ViT cls tokens torch.Size([1, 1, 192])\n",
      "ViT after cat operatiom torch.Size([1, 2305, 192])\n",
      "ViT pos_embed torch.Size([1, 2305, 192])\n",
      "ViT after adding pos embedding torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "ViT Last step of encoder torch.Size([1, 2305, 192])\n",
      "Vision transformer im shape torch.Size([1, 3, 768, 768])\n",
      "Patch Embedding im shape torch.Size([1, 3, 768, 768])\n",
      "Patch embedding After proj operation x shape torch.Size([1, 2304, 192])\n",
      "ViT after patch embed operation torch.Size([1, 2304, 192])\n",
      "ViT cls tokens torch.Size([1, 1, 192])\n",
      "ViT after cat operatiom torch.Size([1, 2305, 192])\n",
      "ViT pos_embed torch.Size([1, 2305, 192])\n",
      "ViT after adding pos embedding torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "ViT Last step of encoder torch.Size([1, 2305, 192])\n",
      "Vision transformer im shape torch.Size([1, 3, 768, 768])\n",
      "Patch Embedding im shape torch.Size([1, 3, 768, 768])\n",
      "Patch embedding After proj operation x shape torch.Size([1, 2304, 192])\n",
      "ViT after patch embed operation torch.Size([1, 2304, 192])\n",
      "ViT cls tokens torch.Size([1, 1, 192])\n",
      "ViT after cat operatiom torch.Size([1, 2305, 192])\n",
      "ViT pos_embed torch.Size([1, 2305, 192])\n",
      "ViT after adding pos embedding torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "torch.Size([1, 2305, 192])\n",
      "ViT Last step of encoder torch.Size([1, 2305, 192])\n",
      "shape of vitembds torch.Size([1, 2305, 192])\n",
      "torch.Size([4, 2305, 192])\n",
      "after rearrange torch.Size([1, 4, 2305, 192])\n",
      "temporal token torch.Size([1, 1, 2305, 192])\n",
      "after cat operation temporal torch.Size([1, 5, 2305, 192])\n",
      "newww torch.Size([1, 11525, 192])\n",
      "torch.Size([1, 11525, 192])\n",
      "torch.Size([1, 11525, 192])\n",
      "torch.Size([1, 11525, 192])\n",
      "torch.Size([1, 11525, 192])\n",
      "torch.Size([1, 11525, 192])\n",
      "torch.Size([1, 11525, 192])\n",
      "torch.Size([1, 11525, 192])\n",
      "torch.Size([1, 11525, 192])\n",
      "torch.Size([1, 11525, 192])\n",
      "torch.Size([1, 11525, 192])\n",
      "torch.Size([1, 11525, 192])\n",
      "torch.Size([1, 11525, 192])\n",
      "torch.Size([1, 11525, 192])\n"
     ]
    }
   ],
   "source": [
    "images =  torch.rand(1,4,3,768,768)\n",
    "num_seqs = images.shape[1]\n",
    "images=images.reshape(4,1,3,768,768)\n",
    "print(images[3].shape)\n",
    "vit_encoder =  VisionTransformer(image_size=(768,768), patch_size=16,n_layers=12,d_model=192,n_cls=10,n_heads=3,d_ff=4*192)\n",
    "vit_encoderoutput=[]\n",
    "for i in range(num_seqs):\n",
    "    x=vit_encoder(images[i])\n",
    "    vit_encoderoutput.append(x)\n",
    "    \n",
    "vit_embds = torch.stack(vit_encoderoutput)\n",
    "\n",
    "vit_embds = vit_embds.reshape(1,4,2305, 192)\n",
    "print(\"shape of vitembds\",vit_embds[:, 0].shape)\n",
    "\n",
    "for idx, emds in enumerate(vit_embds):\n",
    "    print(emds.shape)\n",
    "temporal_token = nn.Parameter(torch.randn(1,1,2305,192))\n",
    "\n",
    "#vit_embds = vit_embds.reshape(4,2305,192)\n",
    "#vit_embds = rearrange(vit_embds[:, 0], \"(b t) ... -> b t ...\", b=1)\n",
    "print(\"after rearrange\",vit_embds.shape)\n",
    "print(\"temporal token\",temporal_token.shape)\n",
    "\n",
    "#cls_temporal_tokens = repeat(temporal_token, \"b t n d -> b t n d\", b=1,t=4)\n",
    "#print(\"cls_temporal\", cls_temporal_tokens.shape)\n",
    "vit_embds = torch.cat((temporal_token, vit_embds), dim=1)\n",
    "print(\"after cat operation temporal\",vit_embds.shape)\n",
    "vit_embds = vit_embds.reshape(1,5*2305,192)\n",
    "print(\"newww\", vit_embds.shape)\n",
    "\n",
    "dpr = [x.item() for x in torch.linspace(0, 0.0, 12)]\n",
    "blocks = nn.ModuleList([Block(192, 3, 4*192, 0.1, dpr[i]) for i in range(12)])\n",
    "#x = self.temporal_transformer(x)\n",
    "        \n",
    "for blk in blocks:\n",
    "    vit_embds = blk(vit_embds)\n",
    "    \n",
    "print(vit_embds.shape)\n",
    "\n",
    "\n",
    "#vit_embds = temporal_transformer(vit_embds)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74caf40d",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
